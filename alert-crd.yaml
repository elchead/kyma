apiVersion: operator.victoriametrics.com/v1beta1
kind: VMRule
metadata:
  name: example-vmalert
spec:
  groups:
    - name: node-exporter.rules
      rules:
        - expr: |
            count without (cpu) (
              count without (mode) (
                node_cpu_seconds_total{job="node-exporter", instance_id="{{ .Values.runtime.instanceID }}"}
              )
            )
          record: instance:node_num_cpu:sum
        - expr: |
            1 - avg without (cpu, mode) (
              rate(node_cpu_seconds_total{job="node-exporter", mode="idle", instance_id="{{ .Values.runtime.instanceID }}"}[1m])
            )
          record: instance:node_cpu_utilisation:rate1m
        - expr: |
            (
              node_load1{job="node-exporter", instance_id="{{ .Values.runtime.instanceID }}"}
            /
              instance:node_num_cpu:sum{job="node-exporter", instance_id="{{ .Values.runtime.instanceID }}"}
            )
          record: instance:node_load1_per_cpu:ratio
        - expr: |
            1 - (
              node_memory_MemAvailable_bytes{job="node-exporter",instance_id="{{ .Values.runtime.instanceID }}"}
            /
              node_memory_MemTotal_bytes{job="node-exporter",instance_id="{{ .Values.runtime.instanceID }}"}
            )
          record: instance:node_memory_utilisation:ratio
    - name: kubernetes-apps
      rules:
        - alert: KubePodCrashLooping
          annotations:
            message:
              Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container
              }}) is restarting {{ printf "%.2f" $value }} times / 5 minutes.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runboomd#alert-name-kubepodcrashlooping
          expr: |
            rate(kube_pod_container_status_restarts_total{job="kube-state-metrics", instance_id="{{ .Values.runtime.instanceID }}"}[15m]) * 60 * 5 > 0
          for: 15m
          labels:
            severity: critical
        - alert: KubePodNotReady
          annotations:
            message:
              Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready
              state for longer than 15 minutes.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runboomd#alert-name-kubepodnotready
          expr: |
            sum by (namespace, pod) (max by(namespace, pod) (kube_pod_status_phase{job="kube-state-metrics"phase=~"Pending|Unknown", instance_id="{{ .Values.runtime.instanceID }}"}) * on(namespace, pod) group_left(owner_kind) max by(namespace, pod, owner_kind(kube_pod_owner{owner_kind!="Job", instance_id="{{ .Values.runtime.instanceID }}"})) > 0
          for: 15m
          labels:
            severity: critical
    - name: kubernetes-storage
      rules:
        - alert: KubePersistentVolumeFillingUp
          annotations:
            message:
              The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
              }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage
              }} free.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefillingup
          expr: |
            kubelet_volume_stats_available_bytes{job="kubelet", metrics_path="/metrics", instance_id="{{ .Values.runtime.instanceID }}"}
              /
            kubelet_volume_stats_capacity_bytes{job="kubelet", metrics_path="/metrics", instance_id="{{ .Values.runtime.instanceID }}"}
              < 0.03
          for: 1m
          labels:
            severity: critical
        - alert: KubePersistentVolumeFillingUp
          annotations:
            message:
              Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim
              }} in Namespace {{ $labels.namespace }} is expected to fill up within four
              days. Currently {{ $value | humanizePercentage }} is available.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefillingup
          expr: |
            (
              kubelet_volume_stats_available_bytes{job="kubelet", metrics_path="/metrics", instance_id="{{ .Values.runtime.instanceID }}"}
                /
              kubelet_volume_stats_capacity_bytes{job="kubelet", metrics_path="/metrics", instance_id="{{ .Values.runtime.instanceID }}"}
            ) < 0.15
            and
            predict_linear(kubelet_volume_stats_available_bytes{job="kubelet", metrics_path="/metrics", instance_id="{{ .Values.runtime.instanceID }}"}[6h], 4 * 24 * 3600) < 0
          for: 1h
          labels:
            severity: warning
        - alert: KubePersistentVolumeErrors
          annotations:
            message:
              The persistent volume {{ $labels.persistentvolume }} has status {{
              $labels.phase }}.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeerrors
          expr: |
            kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics", instance_id="{{ .Values.runtime.instanceID }}"} > 0
          for: 5m
          labels:
            severity: critical
    - name: kubernetes-resources
      rules:
        - alert: KubeCPUOvercommit
          annotations:
            message:
              Cluster has overcommitted CPU resource requests for Pods and cannot
              tolerate node failure.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runboomd#alert-name-kubecpuovercommit
          expr: |
            sum(namespace:kube_pod_container_resource_requests_cpu_cores:sum{instance_id="{{ .Values.runtime.instanceID }}"})
              /
            sum(kube_node_status_allocatable_cpu_cores{instance_id="{{ .Values.runtime.instanceID }}"})
              >
            (count(kube_node_status_allocatable_cpu_cores{instance_id="{{ .Values.runtime.instanceID }}"})-1) / count(kube_node_status_allocatable_cpu_cores{instance_id="{{ .Values.runtime.instanceID }}"})
          for: 5m
          labels:
            severity: warning
        - alert: KubeMemoryOvercommit
          annotations:
            message:
              Cluster has overcommitted memory resource requests for Pods and cannot
              tolerate node failure.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runboomd#alert-name-kubememoryovercommit
          expr: |
            sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum{instance_id="{{ .Values.runtime.instanceID }}"})
              /
            sum(kube_node_status_allocatable_memory_bytes{instance_id="{{ .Values.runtime.instanceID }}"})
              >
            (count(kube_node_status_allocatable_memory_bytes{instance_id="{{ .Values.runtime.instanceID }}"})-1)
              /
            count(kube_node_status_allocatable_memory_bytes{instance_id="{{ .Values.runtime.instanceID }}"})
          for: 5m
          labels:
            severity: warning
        - alert: KubeCPUQuotaOvercommit
          annotations:
            message: Cluster has overcommitted CPU resource requests for Namespaces.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runboomd#alert-name-kubecpuquotaovercommit
          expr: |
            sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="cpu",instance_id="{{ .Values.runtime.instanceID }}"})
              /
            sum(kube_node_status_allocatable_cpu_cores{instance_id="{{ .Values.runtime.instanceID }}"})
              > 1.5
          for: 5m
          labels:
            severity: warning
        - alert: KubeMemoryQuotaOvercommit
          annotations:
            message: Cluster has overcommitted memory resource requests for Namespaces.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runboomd#alert-name-kubememoryquotaovercommit
          expr: |
            sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="memory", instance_id="{{ .Values.runtime.instanceID }}"})
              /
            sum(kube_node_status_allocatable_memory_bytes{job="node-exporter",instance_id="{{ .Values.runtime.instanceID }}"})
              > 1.5
          for: 5m
          labels:
            severity: warning
        - alert: KubeQuotaExceeded
          annotations:
            message:
              Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage
              }} of its {{ $labels.resource }} quota.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runboomd#alert-name-kubequotaexceeded
          expr: |
            kube_resourcequota{job="kube-state-metrics", type="used",instance_id="{{ .Values.runtime.instanceID }}"}
              / ignoring(instance, job, type)
            (kube_resourcequota{job="kube-state-metrics", type="hard",instance_id="{{ .Values.runtime.instanceID }}"} > 0)
              > 0.90
          for: 15m
          labels:
            severity: warning
        - alert: CPUThrottlingHigh
          annotations:
            message:
              "{{ $value | humanizePercentage }} throttling of CPU in namespace
              {{ $labels.namespace }} for container {{ $labels.container }} in pod {{
              $labels.pod }}."
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runboomd#alert-name-cputhrottlinghigh
          expr: |
            sum(increase(container_cpu_cfs_throttled_periods_total{container!="", instance_id="{{ .Values.runtime.instanceID }}"}[5m])) by (container, pod, namespace)
              /
            sum(increase(container_cpu_cfs_periods_total{instance_id="{{ .Values.runtime.instanceID }}"}[5m])) by (container, pod, namespace)
              > ( 25 / 100 )
          for: 15m
          labels:
            severity: warning
    - name: kubernetes-system-apiserver
      rules:
        - alert: KubeAPILatencyHigh
          annotations:
            message:
              The API server has an abnormal latency of {{ $value }} seconds for
              {{ $labels.verb }} {{ $labels.resource }}.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbookmd#alert-name-kubeapilatencyhigh
          expr: |
            (
              cluster:apiserver_request_duration_seconds:mean5m{job="apiserver"}
              >
              on (verb) group_left()
              (
                avg by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job="apiserver",instance_id="{{ .Values.runtime.instanceID }}"} >= 0)
                +
                2*stddev by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job="apiserver",instance_id="{{ .Values.runtime.instanceID }}"} >= 0)
              )
            ) > on (verb) group_left()
            1.2 * avg by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job="apiserver",instance_id="{{ .Values.runtime.instanceID }}"} >= 0)
            and on (verb,resource)
            cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job="apiserver",quantile="0.99",instance_id="{{ .Values.runtime.instanceID }}"}
            >
            1
          for: 5m
          labels:
            severity: warning
        - alert: KubeAPIErrorsHigh
          annotations:
            message:
              API server is returning errors for {{ $value | humanizePercentage
              }} of requests for {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource
              }}.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbookmd#alert-name-kubeapierrorshigh
          expr: |
            sum(rate(apiserver_request_total{job="apiserver",code=~"5..",instance_id="{{ .Values.runtime.instanceID }}"}[5m])) by (resource,subresource,verb)
              /
            sum(rate(apiserver_request_total{job="apiserver",instance_id="{{ .Values.runtime.instanceID }}"}[5m])) by (resource,subresource,verb) > 0.05
          for: 10m
          labels:
            severity: warning
